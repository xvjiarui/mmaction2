apiVersion: batch/v1
kind: Job # Deployment will automatically restart when killed. Use Pod if not needed
metadata:
  labels:
    k8s-app: research
    user: jiarui
  generateName: 'dv-{{job_name}}' # replace <your_name> with something that identifies you
  namespace: '{{name_space}}'
spec:
  ttlSecondsAfterFinished: 86400  # Wait one day to delete completed jobs
  template:
    metadata:
      labels:
        config: '{{base_config}}'
    spec:
      restartPolicy: Never
      containers:
      - name: research
        image: gitlab-registry.nautilus.optiputer.net/xvjiarui0826/mmaction2:latest
        imagePullPolicy: Always
        command: ["/bin/sh"] # replace this with your own job execution scripts
        args: ["-c", "rm -rf mmaction2; git clone -b {{branch}} https://github.com/xvjiarui/mmaction2.git; cd mmaction2;
        python setup.py develop; ln -s /data; mkdir -p /exps/mmaction2/work_dirs; {{link}} {{wandb}}
        ./tools/dist_test_davis.sh {{config}} {{gpus}} {{py_args}}"]
        resources:
          requests: # Minimum resources needed for the job
            memory: "{{mem}}"
            cpu: "{{ cpus }}"
            nvidia.com/gpu: "{{ gpus }}"
          limits: # Maximum resources can be used for the job
            memory: "{{max_mem}}"
            cpu: "{{max_cpus}}"
            nvidia.com/gpu: "{{ gpus }}"
        volumeMounts:
          - mountPath: /data # the directory you can access your persistent storage in container
            name: data
#          - mountPath: /data_zip # the directory you can access your persistent storage in container
#            name: data-zip
          - mountPath: /exps # the directory you can access your persistent storage in container
            name: exps
          - mountPath: /dev/shm # Crucial for multi-gpu or multi processing jobs -> enlarge shared memory
            name: dshm
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
#                  - key: nautilus.io/group
#                    operator: In
#                    values:
#                      - haosu
                  - key: gpu-type
                    operator: In
                    values:
                      - "1070"
                      - "1080"
                      - 1080Ti
                      - 2080Ti
                      - titan-xp
                      - titan-x
                      - TITANRTX
##                      - A100
                      - V100
#                      - "3090"
#                      - RTX8000
                  - key: kubernetes.io/hostname
                    operator: NotIn
                    values:
#                      - k8s-gpu-01.calit2.optiputer.net
#                      - k8s-haosu-07.sdsc.optiputer.net
#                      - ucm-fiona01.ucmerced.edu
#                      - fiona8.ucsc.edu
#                      - k8s-chase-ci-04.calit2.optiputer.net
#                      - k8s-gpu-01.calit2.optiputer.net
#                      - prp-gpu-6.t2.ucsd.edu
#                      - fiona8-3.calit2.uci.edu
#                      - prp-gpu-2.t2.ucsd.edu
#                      - k8s-ravi-01.calit2.optiputer.net
#                      - clu-fiona2.ucmerced.edu
#                      - suncave-1
#                      - k8s-gpu-2.ucr.edu
                      - dtn-gpu2.kreonet.net
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              preference:
                matchExpressions:
                  - key: gpu-type
                    operator: In
                    values:
                      - "1070"
                      - "1080"
                      - 1080Ti
                      - 2080Ti
                      - V100
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data
#        - name: data-zip
#          persistentVolumeClaim:
#            claimName: data-zip
        - name: exps
          persistentVolumeClaim:
            claimName: exps
        - name: dshm
          emptyDir:
            medium: Memory
