apiVersion: batch/v1
kind: Job # Deployment will automatically restart when killed. Use Pod if not needed
metadata:
  labels:
    k8s-app: research
  generateName: action # replace <your_name> with something that identifies you
  namespace: self-supervised-video
spec:
  ttlSecondsAfterFinished: 86400  # Wait one day to delete completed jobs
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: research
        image: gitlab-registry.nautilus.optiputer.net/xvjiarui0826/mmaction2:latest
        imagePullPolicy: Always
        command: ["/bin/sh"] # replace this with your own job execution scripts
        args: ["-c", " git clone -b dev https://github.com/xvjiarui/mmaction2.git; cd mmaction2; python setup.py develop; ln -s /data; mkdir -p /exps/mmaction2/work_dirs; ln -s /exps/mmaction2/work_dirs; ./tools/dist_train.sh configs/representation/walker/space-time-walker_r18_video_8x8x1_50e_kinetics400_rgb.py 4 --validate --seed 0 --auto-resume"]
        resources:
          requests: # Minimum resources needed for the job
            memory: "16Gi"
            cpu: "12"
            nvidia.com/gpu: 2
          limits: # Maximum resources can be used for the job
            memory: "32Gi"
            cpu: "32"
            nvidia.com/gpu: 2
        volumeMounts:
          - mountPath: /data # the directory you can access your persistent storage in container
            name: data
          - mountPath: /exps # the directory you can access your persistent storage in container
            name: exps
          - mountPath: /dev/shm # Crucial for multi-gpu or multi processing jobs -> enlarge shared memory
            name: dshm
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data
        - name: exps
          persistentVolumeClaim:
            claimName: exps
        - name: dshm
          emptyDir:
            medium: Memory
